{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import API_helper.url_module as hp\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "from prefect.deployments import Deployment\n",
    "from prefect import flow,task\n",
    "from google.cloud import storage\n",
    "from config import query_bq \n",
    "from prefect_gcp import GcpCredentials\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential.google_cred\n",
    "\n",
    "\n",
    "@task(name='Get_BQ_SQL',log_prints=True)\n",
    "def GetBQdata(query): \n",
    "    gcp_credentials_block = GcpCredentials.load(\"ete-projectcredential\")\n",
    "    df_bq = pd.read_gbq(query=query,\n",
    "    project_id='ete-projectdatatalkclub',\n",
    "    credentials=gcp_credentials_block.get_credentials_from_service_account()\n",
    "    )\n",
    "    return df_bq\n",
    "\n",
    "\n",
    "@task(log_prints=True)\n",
    "def getApiMap(para_regiontype,para_seriesgroup,para_season,para_unit,para_frequency,para_mindate,para_maxdate):\n",
    "\n",
    "        print('Calling the API.')\n",
    "        result = hp.map.regional(\n",
    "            file_type ='json' \\\n",
    "            ,series_group = para_seriesgroup \\\n",
    "            ,region_type = para_regiontype \\\n",
    "            ,date = para_maxdate \\\n",
    "            ,start_date = para_mindate\n",
    "            ,season=para_season \\\n",
    "            ,units=para_unit\n",
    "            ,frequency = para_frequency\n",
    "        )\n",
    "        \n",
    "        if result.ok:\n",
    "            try:\n",
    "                data_all = result.json()\n",
    "                data_date =data_all['meta']['data']\n",
    "                return data_date    \n",
    "\n",
    "            except Exception:\n",
    "                print(f'An error occured {Exception}')\n",
    "                return None\n",
    "\n",
    "        else:\n",
    "            print(f'An error occur {result.status_code}.')\n",
    "            print(f'An error occur {result.content}.')\n",
    "            print('Waiting for 5 sec.')\n",
    "            return 'retry'\n",
    "\n",
    "@task(log_prints=True)\n",
    "def movearchive(bucket,type):\n",
    "    file_acrhive=  list(bucket.list_blobs())\n",
    "    for file in file_acrhive:\n",
    "        name = file.name\n",
    "        pos = name.find('/')\n",
    "        type = name[:pos]\n",
    "        if type == 'staging':\n",
    "            sub_pos = name.find('/',pos+1)\n",
    "            sub_type = name[pos+1:sub_pos]\n",
    "            if sub_type == type:\n",
    "                bucket.copy_blob(file,destination_bucket=bucket,new_name=name.replace('staging','archive'))\n",
    "                bucket.delete_blob(name)    \n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow(log_prints=True)\n",
    "def test():\n",
    "    # series_parameter = getsqldata(query_sql.query_getMapPara)\n",
    "    series_parameter = GetBQdata(query_bq.query_getMapPara)\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket('fred-etedata')\n",
    "    print('Moving file to archive folder')\n",
    "    movearchive(bucket,'map')\n",
    "    time_stamp = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    for api_para in series_parameter.values.tolist():\n",
    "        para_regiontype  = api_para[0][0]\n",
    "        para_seriesgroup = api_para[1][0]\n",
    "        para_season      = api_para[2][0]\n",
    "        para_unit        = api_para[3][0]\n",
    "        para_frequency   = api_para[4][0]   \n",
    "        para_mindate     = api_para[5]\n",
    "        para_maxdate     = api_para[6]\n",
    "\n",
    "        retry = 0\n",
    "        while retry <3:\n",
    "            data_date =   getApiMap(para_regiontype,para_seriesgroup,para_season,para_unit, para_frequency,para_mindate,para_maxdate)\n",
    "            if data_date == 'retry':\n",
    "                retry+= 1\n",
    "                print(f'Retry the current call. Current retry {retry}')\n",
    "                continue\n",
    "            elif data_date ==None:\n",
    "                print(f'Skpping the call for {para_seriesgroup}')\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        if  retry ==3:\n",
    "            print(f'Error occur skipping {para_seriesgroup}')\n",
    "            continue\n",
    "        time_para = para_maxdate.strftime('%Y-%m-%d')\n",
    "        df_map = pd.json_normalize(data_date[time_para])\n",
    "        \n",
    "        \n",
    "        uppload_path =f'staging/map/{time_stamp}/MapData_{para_seriesgroup}_{time_para}.parquet'\n",
    "        print(f'Uploading file to cloud for {para_seriesgroup}')        \n",
    "        bucket.blob(uppload_path).upload_from_string(df_map.to_parquet(), 'text/parquet')\n",
    "\n",
    "\n",
    "        # retry = 0\n",
    "        # while retry <2:\n",
    "        #     try:\n",
    "        #         bucket.blob(uppload_path).upload_from_string(df_map.to_parquet(), 'text/parquet')\n",
    "        #         continue\n",
    "\n",
    "        #     except Exception as err:\n",
    "        #         retry+=1\n",
    "        #         print(f'An error occur while upload file {err}')\n",
    "        #         print(f'Wait for 5 second and retry. Current retry {retry}')\n",
    "        #         time.sleep(5)\n",
    "        time.sleep(1)\n",
    "    print('Finish running the function.')\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow\n",
    "def crdtest():\n",
    "    from dotenv import load_dotenv\n",
    "    from prefect_gcp import GcpCredentials\n",
    "\n",
    "    basedir=os.getcwd()\n",
    "    load_dotenv(os.path.join(basedir, './.env'))\n",
    "\n",
    "    gcp_credentials_block = GcpCredentials.load(os.getenv(\"Prefect_Credential\")).get_credentials_from_service_account()\n",
    "    return gcp_credentials_block\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = gcp_credentials_block.get_credentials_from_service_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred =crdtest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cred.__dict__.items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_acrhive=  list(bucket.list_blobs())\n",
    "for file in file_acrhive:\n",
    "    name = file.name\n",
    "    pos = name.find('/')\n",
    "    type = name[:pos]\n",
    "    if type == 'staging':\n",
    "        sub_pos = name.find('/',pos+1)\n",
    "        sub_type = name[pos+1:sub_pos]\n",
    "        if sub_type == type:\n",
    "            bucket.copy_blob(file,destination_bucket=bucket,new_name=name.replace('staging','archive'))\n",
    "            bucket.delete_blob(name)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "basedir=os.getcwd()[:-5]\n",
    "load_dotenv(os.path.join(basedir, './.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect_gcp.cloud_storage import GcsBucket\n",
    "\n",
    "@flow(name='my_unique_name')\n",
    "def test():\n",
    "    gcp_cloud_storage_bucket_block = GcsBucket.load(\"ete-project-gcs\")\n",
    "    gcp_cloud_storage_bucket_block.UPLOAD_FROM_DATAFRAME()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect_gcp.credentials import GcpCredentials\n",
    "from prefect import flow,task\n",
    "@flow()\n",
    "def test():\n",
    "    gcp_credentials_block = GcpCredentials.load(\"ete-projectcredential\")\n",
    "    gcp_credentials_block.get_credentials_from_service_account()\n",
    "    print(gcp_credentials_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigquery_client():\n",
    "    gcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\")\n",
    "    google_auth_credentials = gcp_credentials_block.get_credentials_from_service_account()\n",
    "    bigquery_client = bigquery.Client(credentials=google_auth_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GcpCredentials.get_credentials_from_service_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_credentials_block.client_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_credentials_block.service_account_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect_gcp.secret_manager import read_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect_gcp import GcpCredentials\n",
    "gcp_credentials_block = GcpCredentials.load(\"ete-projectcredential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_credentials_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow\n",
    "from prefect_gcp import GcpCredentials\n",
    "from prefect_gcp.secret_manager import read_secret\n",
    "\n",
    "@flow()\n",
    "def example_cloud_storage_read_secret_flow():\n",
    "    gcp_credentials = GcpCredentials.load(\"ete-projectcredential\")\n",
    "    secret_value = read_secret(\"secret_name\", gcp_credentials, version_id=1)\n",
    "    return secret_value\n",
    "\n",
    "example_cloud_storage_read_secret_flow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import API_helper.url_module as hp\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from prefect import flow,task\n",
    "from google.cloud import storage\n",
    "from config import query_bq,clean_df\n",
    "from prefect.deployments import Deployment\n",
    "from prefect_gcp import GcpCredentials\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "@task(log_prints=True)\n",
    "def movearchive(bucket,typecheck):\n",
    "    file_acrhive=  list(bucket.list_blobs())\n",
    "    return file_acrhive\n",
    "    for file in file_acrhive:\n",
    "        name = file.name\n",
    "        pos = name.find('/')\n",
    "        type = name[:pos]\n",
    "        if type == 'staging':\n",
    "            sub_pos = name.find('/',pos+1)\n",
    "            sub_type = name[pos+1:sub_pos]\n",
    "            if sub_type == type:\n",
    "                bucket.copy_blob(file,destination_bucket=bucket,new_name=name.replace('staging','archive'))\n",
    "                bucket.delete_blob(name)    \n",
    "@flow(log_prints=True)\n",
    "def main():\n",
    "    basedir=os.getcwd()\n",
    "    load_dotenv(os.path.join(basedir, './.env'))\n",
    "    gcp_credentials_block = GcpCredentials.load(os.getenv(\"Prefect_Credential\"))\n",
    "\n",
    "    # Get series id base on cat id\n",
    "    client = storage.Client(credentials=gcp_credentials_block.get_credentials_from_service_account())\n",
    "    bucket = client.get_bucket(os.getenv(\"Gcs_Bucket_name\"))\n",
    "    \n",
    "    a =movearchive(bucket,'series')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"Google_Cred_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetBQdata(query):     \n",
    "    df_bq = pd.read_gbq(query=query,\n",
    "    project_id=os.getenv(\"Gcp_Project_id\"),\n",
    "    credentials=gcp_credentials_block.get_credentials_from_service_account()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typecheck ='series'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typecheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in list:\n",
    "        name = file.name\n",
    "        pos = name.find('/')\n",
    "        type = name[:pos]\n",
    "        if type == 'staging':\n",
    "            sub_pos = name.find('/',pos+1)\n",
    "            sub_type = name[pos+1:sub_pos]\n",
    "            if sub_type == typecheck:\n",
    "                print(file)\n",
    "                # bucket.copy_blob(file,destination_bucket=bucket,new_name=name.replace('staging','archive'))\n",
    "                # bucket.delete_blob(name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_dbt_debug('dev')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "601b049b69c015f341cf049bdfa863242bcd463fff4e9ea5ae96948b9f5d87db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
